\section{Information}

\subsection{Information}
 
Über einen Kanal werden Zeichen von der Quelle zur Senke geschickt. Dabei muss
die Senke über den gleichen Zeichenvorrat verfügen wie die Quelle, da sonst
keine Interpretation möglich ist. Ein unbekanntes Zeichen ist irrelevant. In der
Regel produziert die Quelle einzelne Zeichen unterschiedlich oft, was bedeutet
dass die Auftrittswahrscheinlichkeit der Zeichen unterschiedlich ist. 

Kommt nun ein einziges Zeichen öfters vor, so ist dieses Redundant, es liefert
somit keine neue Information. Tritt ein Zeichen dagegen nur sehr selten auf, ist
der Informationsgehalt dieses Zeichens sehr hoch. 

Der \textbf{Informationsgehalt} $I$ eines Zeichens ist der Zweierlogarithmus der
Wahrscheinlichkeit $p$ und wird in Bit angegeben.

\begin{displaymath}
	I(p) = \unaryminus \log_2(p)
\end{displaymath}


\subsection{Entscheidungsgehalt}

Der \textbf{Entscheidungsgehalt} entspricht der Anzahl Elementarentscheidungen.

Eine \textbf{Elementarentscheidung} ist eine zweiwertige Entscheidung, wie
Ja/Nein oder 0/1 und entspricht einem Bit. Mit einer Elementarentscheidung
lassen sich allerdings nur zwei Zeichen codieren. 

Will man $N$ Zeichen unterscheiden sind $\log_2(N)$ Bit erforderlich. Dieser
Entscheidungsgehalt kann auch ungerade Werte annehmen und ist eine abstrakte
Grösse.

\begin{displaymath}
	H_0=\log_2(N)
\end{displaymath}

Der \textbf{Entscheidungsfluss} $H^*_0$ ist der Entscheidungsgehalt über eine
bestimmte Zeit $t$, welche zur Übertragung eines Quellzeichens nötig. ist.

\begin{displaymath}
	H^*_0=\frac{\log_2(N)}{t}
\end{displaymath}


\subsection{Entropie}

Die \textbf{Entropie} $H$ ist in Anlehnung an die Thermodynamik ein Mass für die
Unordnung eines Systems. Werden Zeichen über einen Kanal von einer Quelle zur
Senke geschickt, so entspricht die Entropie der Ungewissheit ein bestimmtes
Zeichen vorherzusagen. Erzeugt die Quelle ein bestimmtes Zeichen sehr häufig, so
die Ungewissheit entsprechend gering. Mit einer bestimmten Wahrscheinlichkeit
lässt sich so ein Zeichen voraussagen. Ist die Häufigkeit der Zeichen jedoch
ausgeglichen, so ist die Ungewissheit beziehungsweise die Entropie maximal. 

Die Entropie einer diskreten, gedächtnislosen Quelle $X$ entspricht dabei dem
durchschnittlichen Informationsgehalt der Zeichen ($x_1$, $x_2$, $x_3$, \ldots,
$x_N$) entsprechend ihrer Auftrittswahrscheinlichkeiten ($p_1$, $p_2$, $p_3$,
\ldots, $p_N$). 

\begin{align*}
H(X)& =\sum\limits_{i=1}^N p_i \cdot I(x_i) \\
& =\sum\limits_{i=1}^N p_i \cdot \log_2\left(p(x_i)\right)
\end{align*}


\subsection{Redundanz}

Die Redundanz einer Quelle ist die Differenz zwischen deren Entropie und dem
Entscheidungsgehalt ihres Zeichenvorrats.

Die \textbf{Redundanz} wird folgendermassen berechnet:
\[
	R = H_0 - H(X)
\]
Die \textbf{Relative Redundanz} einer Quelle $X$ mit Entscheidungsgehalt $H_0$
ist:
\[
	r = \frac{R}{H_0} = 1 - \frac{H(X)}{H_0}
\]


\subsection{Quelle mit Gedächtnis}

Die bisherigen Quellen arbeiten ohne Gedächtnis. Allgemein kann nicht von einer
gedächtnislosen Quelle ausgegangen werden. Daher benötigen wir für die
Berechnungen die Verbundentropie der Zeichen.

Verbundwahrscheinlichkeit:
\begin{align*}
	&P(x_i, y_k) = P(x_i) \cdot P(y_k) \\
	&P(x_i, y_l) = P(x_i) \cdot P(y_k|x_i)
\end{align*}

Verbundentropie zweier Zeichen einer gedächtnisbehafteten Quelle:
\begin{align*}
	&H \langle X,Y \rangle = H \langle X \rangle + H \langle Y|X \rangle \\
	&H(X,Y) = \unaryminus \sum_X \sum_Y p(x,y) \cdot \log_2\left(p(x,y)\right)
\end{align*}

Wichtiges Merkmal: Die Mittlere Entropie einer Quelle \textbf{ohne} Gedächtnis
ist immer \textbf{grösser oder gleich} der Entropie einer Quelle \textbf{mit}
Gedächtnis.

Bei einer Quelle mit Gedächtnis können wir Voraussagen treffen, welches Zeichen
als nächstes kommt, daher sinkt der Informationsgehalt der Nachricht.
