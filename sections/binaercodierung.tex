\section{Binärcodierung}

% TODO Huffmann, Markov, etc\ldots


\subsection{Diskrete Quellen}

Bei den diskreten Quellen werden beliebige Zeichen der Quelle auf binäre
Codeworte ($CW$) abgebildet.

Günstig ist, wenn die mittlere Codewortlänge $L$ möglichst klein ist.

Beispiele für $CW$:
\begin{itemize}
	\item ASCII: Block-Code mit fester Wortgrösse ($L = 8 \: Bit$)
	\item Morsecode: Variable Wortgrösse ($L = 1 \ldots 4 \: Bit$)
\end{itemize}

Mittlere Codewortlänge:
\[
	L = \sum_{i=1}^n p(x_i) \cdot L(x_i) \left[\frac{Bit}{Zeichen}\right]
\]


\subsection{Präfixeigenschaft}
\label{sec:praefixeigenschaft}

Der Morsecode besitzt das Problem, dass er zwischen den einzelnen Zeichen eine
Pause bzw. ein Trennzeichen benötigt. Fehlt dieses Trennzeichen, so kann der
Empfänger nicht entscheiden, ob ein gültiges Zeichen oder nur ein
Zwischenschritt zu einem anderen gültigen Zeichen gesendet wurde.

Ein Code besitzt dann die Präfixeigenschaft, wenn alle Zeichen ausschliesslich
in den Blättern des Baumes codiert sind.  

Für jede Codierung mit Präfixeigenschaft gilt: Die Entropie $H$ ist stets
kleiner oder gleich gross wie die mittlere Codewortlänge $L$.

\[
	H(x) \le L(X)
\]

\begin{figure}[H]
	\fbox{%
		\begin{minipage}{.4\linewidth}
			\input{tikz/praefixeigenschaft}
		\end{minipage}
		\begin{minipage}{.6\linewidth}
			\begin{tabular}{@{}l l}
				A = 100 & p = 0.2 \\
				B = 1101 & p = 0.1 \\
				C = 1100 & p = 0.1 \\
				D = 111 & p = 0.2 \\
				E = 0 & p = 0.4 \\
			\end{tabular}\\\\

			\textit{CEBA}: 1100 / 0 / 1101 / 100 \\

			$L = 3 \cdot 0.2 + 4 \cdot 0.1 + 4 \cdot 0.1 + 3 \cdot 0.2 + 1 \cdot 0.4 = 2.4$

		\end{minipage}
	}
	\caption{Beispielcode mit Präfixeigenschaft}
\end{figure}


\subsection{Quelle mit Gedächtnis}

Die bisherigen Quellen arbeiten ohne Gedächtnis. Allgemein kann nicht von einer
gedächtnislosen Quelle ausgegangen werden. Daher benötigen wir für die
Berechnungen die Verbundentropie der Zeichen.

Verbundwahrscheinlichkeit:
\begin{align*}
	&P(x_i, y_k) = P(x_i) \cdot P(y_k) \\
	&P(x_i, y_l) = P(x_i) \cdot P(y_k|x_i)
\end{align*}

Verbundentropie zweier Zeichen einer gedächtnisbehafteten Quelle:
\[
	H \langle X,Y \rangle = H \langle X \rangle + H \langle Y|X \rangle
\]

Wichtiges Merkmal: Die Mittlere Entropie einer Quelle \textbf{ohne} Gedächtnis
ist immer \textbf{grösser oder gleich} der Entropie einer Quelle \textbf{mit}
Gedächtnis.

Bei einer Quelle mit Gedächtnis können wir Voraussagen treffen, welches Zeichen
als nächstes kommt, daher sinkt der Informationsgehalt der Nachricht.
